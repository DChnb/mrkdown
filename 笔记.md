##### 选择题

> 6 $\times$ 3'

- ###### **凸函数**

  **凸函数**（英文：Convex function）是指函数图形上，==任意两点连成的线段，皆位于图形的上方的实值函数==，如单变数的二次函数和指数函数。二阶可导的一元函数为凸，当且仅当其定义域为凸集，且函数的二阶导数在整个定义域上非负。直观理解，凸函数的图像形如开口向上的杯∪。

  ==在最优化研究中，凸函数的最小化问题有唯一性==，即凸开集上的严格凸函数，至多只有一个极小值。

  1. 根据函数的定义，计算其一阶和二阶导数，然后证明对于函数定义域内的所有点，二阶导数非负。如果二阶导数非负，就说明该函数是凸函数。

  2. 凸集：这表示集合中的所有点的线性组合仍然属于该集合。凸集的一个直观理解是，集合内的所有点之间的连线都在集合内部。

     

- ###### **激活函数**

  - Sigmoid
  
  - Relu
  
    
  
- ###### **损失函数**

  - **交叉熵**

    $H(y,p)=-\left(y\cdot\log(p)+(1-y)\cdot\log(1-p)\right)$

    
  
  - **KL 散度**
  
    $D_{\mathrm{KL}}(P\|Q)=\sum_iP(i)\cdot\log\left(\frac{P(i)}{Q(i)}\right)$
    
    

##### 计算题

> 10' + 12' + 10'

- ###### **线性回归**

  - **一元线性回归**

    线性回归试图学得

    ​	$f(x_i)=wx_i+b,\text{ 使得 }f(x_i)\simeq y_i.$

    如何确定 $w$ 和 $b$ 呢? 关键在于如何衡量 $f(x)、y$ 之间的差别,

    这里可以使用均方误差 因此我们可试图让均方误差最小化，即

    ​	$\begin{aligned}
    (w^*,b^*)& =\arg\min_{(w,b)}\sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2  \\
    &=\arg\min_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2~.
    \end{aligned}$

    求导可得

    ​	$\begin{aligned}
    &\frac{\partial E_{(w,b)}}{\partial w} =2\left(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m\left(y_i-b\right)x_i\right),  \\
    &\frac{\partial E_{(w,b)}}{\partial b} =2\left(mb-\sum_{i=1}^m\left(y_i-wx_i\right)\right), 
    \end{aligned}$

    令上式为零得到最优解

    ​	$\begin{aligned}
    w=\frac{\sum\limits_{i=1}^my_i(x_i-\bar{x})}{\sum\limits_{i=1}^mx_i^2-\frac1m\left(\sum\limits_{i=1}^mx_i\right)^2}, \\\\
    b=\frac{1}{m}\sum_{i=1}^{m}(y_{i}-wx_{i}),
    \end{aligned}$

    

  - **多元线性回归**

    数据集形式

    ​	$\mathbf{X}=\begin{pmatrix}x_{11} & x_{12} & \ldots & x_{1d} & 1\\ x_{21} & x_{22} & \ldots & x_{2d} & 1\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ x_{m1} & x_{m2} & \ldots & x_{md} & 1\end{pmatrix}=\begin{pmatrix}{x}_1^{\mathrm{T}} & 1\\ {x}_2^{\mathrm{T}} & 1\\ \vdots & \vdots\\ {x}_{m}^{\mathrm{T}} & 1\end{pmatrix}$

    ​	多一列 1 是为了对应参数向量 $\hat{\boldsymbol{w}}=(\boldsymbol{w};b)$

    优化目标

    ​	$\hat{\boldsymbol{w}}^*=\arg\min_{\hat{\boldsymbol{v}}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right)^{\mathrm{T}}\left(\boldsymbol{y}-\mathbf{X}\hat{\boldsymbol{w}}\right).$

    求导

    ​	$\frac{\partial E_{\hat{\boldsymbol{w}}}}{\partial\hat{\boldsymbol{w}}}=2\mathbf{ X}^{\mathrm{T}}\left(\mathbf{X}\hat{\boldsymbol{w}}-\boldsymbol{y}\right)$

    令上式为零，得最优解

    ​	$\hat{\boldsymbol{w}}^*=\left(\mathbf{X}^\mathrm{T}\mathbf{X}\right)^{-1}\mathbf{X}^\mathrm{T}\boldsymbol{y},$

    

- ###### **神经网络**

  <img src="笔记.assets/image-20240111215322067.png" alt="image-20240111215322067" style="zoom:35%;float:left" />

  - 更新参数公式

    $w^+=w-\eta\cdot\frac{\partial E}{\partial w}$

  - 更新参数 $w_1$

    $\frac{\partial E}{\partial w_1}=\frac{\partial E}{\partial y}\cdot\frac{\partial y}{\partial h_1}\cdot\frac{\partial h_1}{\partial w_1}$

    

- ###### **支持向量机**

  - 基本问题:

    $\begin{aligned}\min_{\boldsymbol{w},b}&\frac12\|\boldsymbol{w}\|^2\\\mathrm{s.t.~}&y_i(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b)\geqslant1,\quad i=1,2,\ldots,m.\end{aligned}$

    

  - 添加拉格朗日乘子化为拉格朗日函数：

    $L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac12\left.\|\boldsymbol{w}\|^2+\sum_{i=1}^m\alpha_i\left(1-y_i(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b)\right)\right.$

    

  - 令 $L(\boldsymbol{w},b,\boldsymbol{\alpha})$ 的 $w$ 和 $b$ 的偏导为零:

    $\begin{aligned}\boldsymbol{w}&=\sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i,\\0&=\sum_{i=1}^m\alpha_iy_i.\end{aligned}$

    

  - 带入上式得对偶问题：

    $\begin{aligned}\max_{\boldsymbol{\alpha}}&\quad\sum_{i=1}^m\alpha_i-\frac12\quad\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_j\\\\\mathrm{s.t.}\quad&\quad\sum_{i=1}^m\alpha_iy_i=0,\\&\quad\alpha_i\geqslant0,\quad i=1,2,\ldots,m.\end{aligned}$

    

  - 零上式导数为零解出 $\boldsymbol{\alpha}$ ：

    $\begin{aligned}f(\boldsymbol{x})&=\boldsymbol{w}^\mathrm{T}\boldsymbol{x}+b\\&=\sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}+b.\end{aligned}$

    

  - 注意上述求解过程需满足 KKT 条件：

    $\left.\left\{\begin{array}{l}\alpha_i\geqslant0;\\y_if(\boldsymbol{x}_i)-1\geqslant0;\\\alpha_i\left(y_if(\boldsymbol{x}_i)-1\right)=0.\end{array}\right.\right..$

    

- ###### **感受野**

  $\begin{aligned}&\\& r_{l}=r_{l-1}+\left((k_{l}-1)*\prod_{i=1}^{l-1}s_{i}\right)\end{aligned}$

  - $r_l$ 为第 $l$ 层感受野

  - $k_l$ 为第 $k$ 层卷积核的大小

  - $s_i$ 为第 $i$ 层步长，注意只算到 $l-1$ 层 (当前层的感受野当然不受本层卷积核大小的影响)

    

- ###### **特征图大小**

  输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为： 

  - (200 + 2 - 5) / 2 + 1 = 99

  - (99 -3) / 1  + 1 = 97

  - (97 + 2 - 3) / 1 + 1 = 99
  
    
  
- ###### **混淆矩阵**
  
  <img src="笔记.assets/image-20240111221742607.png" alt="image-20240111221742607" style="zoom:50%;float:left" />
  
- ###### **逻辑回归**

  - 检测边界：

    $h_\theta=g(\theta_0+\theta_1x_1+\theta_2x_2)$

    

  - 目标：

    $J(\theta)=-\left[\frac1m\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log\left[1-h_\theta(x^{(i)})\right]\right]+\frac\lambda m\sum_{j=1}^n\theta_j^2$

    

  - 梯度下降算法：

    $\begin{aligned}\theta_0{:}&=\theta_0-\alpha\frac1m\sum_{i=1}^m\left[h_\theta\left(x^{(i)}\right)-y^{(i)}\right]x_0^{(i)}\\\theta_j{:}&=\theta_j-\alpha{\left[\frac1m\sum_{i=1}^m\left[h_\theta\left(x^{(i)}\right)-y^{(i)}\right]x_j^{(i)}+\frac\lambda m\theta_j\right.}\end{aligned}$

  

- ######  **留 1 训练法, 交叉验证法**

  **留一法（Leave-One-Out Cross-Validation，LOOCV）：**

  1. 对于有 *N* 个样本的数据集，进行 *N* 次迭代。
  2. 在每次迭代中，将一个样本作为验证集，其余 *N*−1 个样本作为训练集。
  3. 在训练集上训练模型，并在验证集上评估模型性能。
  4. 重复上述步骤 *N* 次，每次都选择不同的验证集。
  5. 最终，计算所有迭代的性能指标的平均值，用于评估模型性能。

  LOOCV的优点是充分利用了数据，每个样本都有机会作为验证集，但计算成本相对较高，特别是对于大型数据集。

  

  **交叉验证法（K-Fold Cross-Validation）：**

  1. 将数据集划分为 *K* 个互斥的子集，称为折叠（folds）。
  2. 进行 *k* 次迭代，每次选择一个折叠作为验证集，其余 −1*k*−1 个折叠作为训练集。
  3. 在训练集上训练模型，并在验证集上评估模型性能。
  4. 重复上述步骤 *k* 次，每次都选择不同的验证折叠。
  5. 最终，计算所有迭代的性能指标的平均值，用于评估模型性能。

  交叉验证法相比LOOCV的计算成本较低，但也能够较好地评估模型的性能。

  这两种方法都有助于减小由于数据划分不同而引起的模型性能评估的方差，提高对模型性能的可靠性评估。选择使用哪种方法通常取决于数据集的大小和计算资源的限制。

  

##### 简答题

> 10 $\times$ 5'

- ###### **简述 Kmeans 主要内容，简要说明 k 参数该如何选择**

  K 均值（K-means）是一种常用的聚类算法，其主要目标是将数据集划分为 K 个不同的簇，使得每个数据点都属于与其最近的簇中心。以下是 K 均值主要的内容：

  1. **初始化聚类中心：** 随机选择 K 个数据点作为初始的聚类中心。
  2. **分配数据点：** 将每个数据点分配到距离其最近的聚类中心所在的簇。
  3. **更新聚类中心：** 计算每个簇的新中心，将中心更新为该簇所有数据点的平均值。
  4. **重复步骤2和3：** 反复进行数据点分配和中心更新，直到收敛或达到预定的迭代次数。
  5. **收敛：** 当簇的分配不再发生变化或达到预定的迭代次数时，算法收敛。

  选择K值是K均值算法中一个重要的问题。一般来说，选择不同的K值可能导致不同的聚类结果。
  有一些常见的方法来选择K：

  1. **肘部法（Elbow Method）：**计算不同K值下的聚类结果的误差平方和（SSE，Sum of Squared Errors），然后选择使得SSE下降趋势明显变缓的K值。图像呈现肘部形状，肘部对应的K值即为选择的簇数。

  2. **轮廓系数法（Silhouette Method）：**计算不同K值下的轮廓系数，选择轮廓系数最大的K值。轮廓系数衡量了簇内的相似度和簇间的差异性。
     
  3. **Gap统计量法（Gap Statistics）：** 比较聚类结果的SSE与随机数据的SSE，选择使得Gap统计量最大的K值。

  4. **专业知识**： 在实际应用中，有时候根据领域专业知识或先验信息来选择K值。

  总的来说，K的选择通常是一个试错的过程，结合多种方法来确定最合适的K值。

    

- ###### **什么是模式识别，研究它的意义**

  模式识别是一种涉及从数据中识别、分类或提取模式的计算机科学和机器学习领域。具体而言，模式识别的目标是通过自动化或半自动化的方式，从大量数据中学习和识别出规律、趋势或特征，从而使计算机系统能够对未知数据进行预测、分类或判别。

  研究模式识别的意义体现在多个方面：

  1. **自动化决策：** 模式识别使得计算机系统能够自动地从大规模数据中学习和提取信息，从而实现自动化决策。这在医学、金融、制造业等领域中具有重要的应用，可以帮助人们做出更准确、高效的决策。
  2. **数据挖掘：** 模式识别技术有助于从海量数据中挖掘出隐藏在其中的有用信息。通过识别数据中的模式，可以发现数据之间的关联性，为企业和研究机构提供更深入的洞察。
  3. **人工智能和机器学习：** 模式识别是机器学习和人工智能领域的基础之一。通过训练算法来识别模式，使计算机系统具备学习和适应能力，从而能够应对不断变化的环境和任务。
  4. **计算机视觉：** 在计算机视觉领域，模式识别用于识别图像或视频中的对象、人脸、文字等。这在人脸识别、图像分类、自动驾驶等应用中具有广泛的应用。
  5. **信号处理：** 模式识别技术在信号处理领域中被用于分析和识别各种信号，如语音、图像、生物信号等，为各种应用提供支持。

  总体而言，模式识别的研究对于提高计算机系统的智能化、自动化水平，以及发现数据中的规律和知识具有重要的意义。在当今大数据时代，模式识别技术的不断发展和应用推动了科技的进步和创新。

  

- ###### **模式识别预处理有哪些基本内容**

  模式识别预处理是在进行模式识别任务之前对原始数据进行的一系列处理步骤，旨在提高数据质量、减少噪音、突出关键特征，以便后续的模型训练和分类。以下是模式识别预处理的一些基本内容：

  1. **数据清洗：** 检测和纠正数据中的错误、异常值或缺失值，以确保输入数据的质量。
  2. **数据归一化/标准化：** 将不同特征的数值范围调整为一致，防止某些特征对模型的影响过大。
  3. **特征选择：** 选择最具代表性和相关性的特征，减少冗余信息，提高模型的效率和性能。
  4. **特征提取：** 通过数学变换或其他方法从原始数据中提取更具代表性的特征，降低数据的维度。
  5. **图像预处理：** 对图像数据进行平滑、增强、边缘检测等操作，以提取有用的图像特征。
  6. **文本处理：** 对文本数据进行分词、词干提取、停用词去除等处理，以获取更具代表性的文本特征。
  7. **数据增强：** 在训练数据中引入一些变化，如旋转、翻转等，以扩充训练集，提高模型的泛化能力。
  8. **去除不一致性：** 处理由于不同传感器、设备等导致的数据不一致性，确保数据的一致性和可比性。
  9. **缺失值处理：** 对于包含缺失值的数据，可以通过填充、插值等方法进行处理，以保持数据的完整性。

  这些预处理步骤的选择和顺序可能取决于具体的模式识别任务和数据类型。预处理的目标是为模型提供更干净、更有代表性的数据，以便模型能够更好地学习并表现出更好的性能。

  

- ###### **简述 SVM 基本思想**

  支持向量机（Support Vector Machine，SVM）是一种用于分类和回归分析的监督学习模型。其基本思想是通过找到能够正确划分不同类别的决策边界，同时最大化类别间的间隔，使得模型在新数据上的泛化能力较强。

  以下是支持向量机的基本思想：

  1. **寻找最佳的决策边界：** SVM的目标是找到一个超平面，将不同类别的样本分隔开。对于二分类问题，这个超平面可以被描述为一个线性方程：$wx+b=0$ 其中 $w$ 是法向量，$x$是样本的特征向量，$b$ 是偏置项。
  2. **最大化间隔：** SVM希望找到的决策边界具有最大的间隔，即样本点到决策边界的距离最大。这样做的目的是提高模型的泛化能力，使其对新样本的分类效果更好。
  3. **支持向量：** 支持向量是离决策边界最近的样本点。它们对决策边界的位置有重要影响，因为它们决定了间隔的大小。
  4. **核函数：** SVM可以通过核函数将数据从原始特征空间映射到高维特征空间，从而在高维空间中找到更为有效的决策边界。常用的核函数包括线性核、多项式核和径向基函数（RBF）核等。
  5. **软间隔：** 在实际应用中，数据可能不是线性可分的，因此引入了软间隔的概念，允许一些样本点出现在决策边界的错误一侧。通过引入惩罚项，SVM能够在权衡间隔最大化和误分类点的数量之间找到平衡。

  总的来说，SVM的基本思想是通过寻找最佳的超平面来将不同类别的样本分开，并通过最大化间隔来提高模型的泛化能力。核函数的引入使得SVM在高维空间中也能处理非线性可分的情况。

  

- ###### 神经网络训练时是否可以将所有参数初始化为零，为什么

  在神经网络中，将所有参数初始化为零是不推荐的，因为这会导致对称权重问题（Symmetry Problem）。对称问题是指在网络的同一层中，所有的权重都被初始化为相同的值，导致同一层内的神经元在训练过程中始终保持相同的行为。具体来说在反向传播过程中，同一层的梯度都将是相同的，而权重初始值又是相同的，致使同一层所有参数始终都是一样的，导致权重矩阵在训练过程中保持对称性，这限制了网络的表达能力，使其难以学习到多样化和复杂的特征。

  为了避免这种情况，通常使用随机初始化的方法，例如从服从正态分布或均匀分布的小随机值中进行抽样。这样每个神经元的权重都是独立的，有了差异性，有助于打破对称性，使得网络能够更好地学习到数据中的特征。

- ###### Sigmoid 的基本步骤，它作为激活函数有什么作用，优缺点是什么

  Sigmoid函数是一种常用的激活函数，通常用于神经网络中。下面是Sigmoid函数的基本步骤、作用，以及它的优缺点：

  **Sigmoid函数基本步骤：**

  1. **数学形式：** Sigmoid函数的数学形式为 $\sigma(x)=\frac1{1+e^{-x}}$, 其中 $e$ 是自然对数的底数。
  2. **输入范围：** Sigmoid函数的输入 $x$ 在负无穷到正无穷之间，但输出范围始终在 0 到 1 之间。
  3. **特性：** Sigmoid函数的输出是一个 S 形曲线，随着输入的增加，输出逐渐趋近于 1，而随着输入的减小，输出逐渐趋近于 0。

  **Sigmoid作为激活函数的作用：**

  1. **非线性映射：** Sigmoid 函数的主要作用是引入非线性映射，使神经网络能够学习复杂的非线性关系。这对于解决更复杂的问题非常重要，因为线性模型的表达能力有限。
  2. **输出概率：** 在二分类问题中，Sigmoid 函数常被用于输出层，将网络的原始输出映射到 0 到 1 的范围，代表样本属于正类别的概率。

  Sigmoid函数的优缺点：

  优点：

  1. **平滑性：** Sigmoid函数是光滑可导的，这使得梯度下降等优化算法更容易进行。
  2. **输出范围：** 输出范围在0到1之间，适用于需要将输出解释为概率的场景，如二分类问题。

  缺点：

  1. **梯度饱和：** Sigmoid函数在输入较大或较小的情况下，梯度接近于零，这可能导致梯度消失问题，使得网络难以学习。这在深度神经网络中可能会影响训练效果。
  2. **不是零中心：** Sigmoid函数的输出不是零中心的，这可能导致神经网络的权重更新不够高效。
  3. **输出值不是以零为中心：** Sigmoid函数输出的均值不是零，这可能导致在反向传播时，一些神经元的梯度总是正的，另一些总是负的，使得参数的更新不够对称。

  由于这些缺点，近年来在深度学习中，一些其他的激活函数如ReLU（Rectified Linear Unit）、Leaky ReLU等被提出，以缓解梯度饱和和梯度消失等问题。

  

- ###### 最大似然估计方法与贝叶斯估计方法的差异


  最大似然估计（Maximum Likelihood Estimation，MLE）和贝叶斯估计（Bayesian Estimation）是两种常用的**参数估计**方法，它们在理论和应用上有很大的差异。

  **最大似然估计（MLE）**：

  1. **目标：** MLE的目标是通过观察到的数据，选择参数的值，使得观察到这些数据的出现概率最大。即，找到==最大化似然函数==的参数值。

  2. **基本思想：** MLE认为模型中的参数是固定的但未知的，且真实值是能够最大化样本数据出现的概率的值。这种方法不考虑参数本身的分布信息，只关注数据的分布。

  3. **公式：** 对于给定的数据集，似然函数 $L(\theta|\text{data})$ 表示在给定参数 $\theta$ 下观察到数据的概率。MLE的估计值是使似然函数最大的参数值：$\hat{\theta}_\mathrm{MLE}=\arg\max_\theta L(\theta|\text{data})$

     

  **贝叶斯估计（Bayesian Estimation）**：

  1. **目标：** 贝叶斯估计的目标是通过引入先验分布，结合观察到的数据，计算参数的后验分布，从而得到参数的估计。
  2. **基本思想：** 贝叶斯估计将参数视为随机变量，认为参数在估计前具有一个先验分布，观察到数据后，通过贝叶斯定理更新参数的分布，得到后验分布。
  3. **公式：** 对于给定的数据集，贝叶斯估计考虑的是参数的后验分布，公式如下：$P(\theta|\text{data})=\frac{P(\text{data}|\theta)\cdotp P(\theta)}{P(\text{data})}$ 其中， $P(\text{data}|\theta)$ 是似然函数，$P(\theta)$ 是先验分布， $P(data)$ 是边缘似然。贝叶斯估计的估计值通常是后验分布的期望值或最可能值。

  **主要差异**：

  1. **角度：** MLE是频率派的观点，认为参数是固定但未知的；贝叶斯估计是贝叶斯派的观点，认为参数是随机的，具有先验分布。
  2. **输出：** MLE输出的是一个具体的点估计值；贝叶斯估计输出的是一个参数的分布，包含了不确定性信息。
  3. **先验信息：** MLE不考虑参数的先验信息，只依赖于观察到的数据；贝叶斯估计引入了先验分布，考虑了参数的先验信息。
  4. **推断：** MLE通过最大化似然函数进行参数估计；贝叶斯估计通过贝叶斯定理结合先验分布和似然函数进行参数推断。

​    

  选择使用MLE还是贝叶斯估计通常取决于问题的性质以及研究者对参数的理解和信念。

  

- ###### 简述 k 邻近分类方法 

  K近邻（K-Nearest Neighbors，简称KNN）是一种常用的分类方法，也可以用于回归问题。它的基本思想是通过测量不同样本点之间的距离，来判断新样本点属于哪个类别。KNN的基本步骤如下：

  KNN的基本步骤：

  1. **选择K值：** 确定用于分类的邻居数K，K是一个正整数，通常选择一个较小的奇数，以防止平局的情况。
  2. **计算距离：** 对于每个待分类的样本点，计算它与训练集中所有样本点的距离。常用的距离度量包括欧氏距离、曼哈顿距离、闵可夫斯基距离等。
  3. **选择K个最近邻：** 从计算出的距离中选取距离最近的K个样本点作为该待分类样本的邻居。
  4. **投票表决：** 统计K个最近邻中各类别的数量，将待分类样本分配给数量最多的类别。对于分类问题，采用多数表决的方式确定最终的类别。

  KNN的优缺点：

  优点：

  1. **简单易理解：** KNN是一种直观的算法，易于理解和实现。
  2. **对异常值不敏感：** KNN对异常值不敏感，因为它是基于邻近样本的统计信息进行分类。
  3. **适用于多类别问题：** KNN适用于多类别问题，并且不需要对数据做假设。

  缺点：

  1. **计算复杂度高：** 随着样本量的增加，计算距离的复杂度会增加，导致算法变得较慢。
  2. **需要大量内存：** KNN需要存储全部训练数据，占用大量内存。
  3. **对特征值范围敏感：** 如果不同特征的尺度差异很大，需要进行特征缩放，否则距离计算可能会被某些特征主导。
  4. **参数K的选择：** K的选择可能影响分类的结果，需要进行调优。

  总体而言，KNN是一个简单而直观的分类算法，适用于小型数据集和相对简单的分类问题。

  

- ###### 神经网络成功快速应用的例子，举四个例子

  1. ChatGPT

  2. 图像生成

  3. 无人驾驶

  4. 语音识别转文字

  5. 推荐系统

  6. 机器翻译

  7. 手机、汽车等智慧助手

     

- ###### 简述隐马尔可夫模型的三个核心问题

  隐马尔可夫模型（HMM）的三个核心问题如下：

  1. **概率计算问题**：给定模型的参数（转换概率矩阵A，观测概率矩阵B，初始状态概率向量π），以及观测序列O，求出观测序列O出现的概率。这个问题的意义在于检测观察到的结果和已知的模型是否一致。
  2. **学习问题**：给定观测序列O，求解模型的参数（转换概率矩阵A，观测概率矩阵B，初始状态概率向量π），使得在给定模型下观测序列O出现的概率最大。这个问题的目标是用已知的可见状态序列去估计HMM模型中的参数。
  3. **预测问题**：给定模型的参数（转换概率矩阵A，观测概率矩阵B，初始状态概率向量π），以及观测序列O，求出最可能的隐藏状态序列I。这个问题的目标是获取一条最匹配的隐马尔科夫链。

  这三个问题构成了隐马尔可夫模型的基础，它们分别对应了概率计算、参数学习和序列预测这三个关键任务。

  

